---
apiVersion: v1
kind: Namespace
metadata:
  name: uaa
---
apiVersion: v1
kind: Namespace
metadata:
  name: korifi-installer
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/enforce: restricted
  name: cf
---
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/enforce: restricted
  name: korifi
---
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJsb2NhbHJlZ2lzdHJ5LWRvY2tlci1yZWdpc3RyeS5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsOjMwMDUwIjp7InVzZXJuYW1lIjoidXNlciIsInBhc3N3b3JkIjoicGFzc3dvcmQiLCJhdXRoIjoiZFhObGNqcHdZWE56ZDI5eVpBPT0ifX19
kind: Secret
metadata:
  name: image-registry-credentials
  namespace: cf
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: korifi-installer
  namespace: korifi-installer
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: korifi-installer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: korifi-installer
    namespace: korifi-installer
---
apiVersion: batch/v1
kind: Job
metadata:
  name: install-uaa
  namespace: korifi-installer
spec:
  template:
    metadata:
      name: install-uaa
    spec:
      containers:
        - name: install-uaa
          image: dtzar/helm-kubectl:3.17  # Keep this image for kubectl and apk
          command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail

              # Install required tools
              apk add --no-cache ca-certificates openssl util-linux curl

              # Download and install ytt
              curl -L -o /usr/local/bin/ytt https://github.com/carvel-dev/ytt/releases/download/v0.51.1/ytt-linux-amd64
              chmod +x /usr/local/bin/ytt

              # Download and install kapp
              curl -L -o /usr/local/bin/kapp https://github.com/carvel-dev/kapp/releases/download/v0.64.0/kapp-linux-amd64
              chmod +x /usr/local/bin/kapp

              function show_file() {
                if [ -z "$1" ]; then
                echo "Error: No filename provided"
                echo "Usage: show_file <filename>"
                return 1
                fi

                if [ ! -f "$1" ]; then
                echo "Error: File '$1' not found"
                return 1
                fi

                local filename="$1"
                echo "--- Contents of $(realpath "$filename") ---"
                echo ""
                cat "$filename"
                echo ""
                echo "----------"
                echo ""
              }

              cd /tmp

              # Generate JWT keys
              openssl genrsa -out jwt_private.pem 2048
              openssl rsa -in jwt_private.pem -pubout -out jwt_public.pem

              show_file jwt_private.pem
              show_file jwt_public.pem

              # Generate SAML keys
              SUBJ="/C=US/ST=CA/L=San Francisco/O=Cloud Foundry/OU=UAA/CN=uaa-saml-key"
              openssl req -x509 -newkey rsa:2048 -keyout saml_private.pem \
                -out saml_cert.pem -days 3650 -nodes \
                -subj "${SUBJ}" \
                -sha256 -extensions v3_req \
                -config <(printf "[req]\ndistinguished_name=req\n[v3_req]\nbasicConstraints=CA:FALSE\nkeyUsage=digitalSignature,keyEncipherment\nextendedKeyUsage=serverAuth,clientAuth")

              show_file saml_private.pem

              # Convert private key to PKCS#8 format
              openssl pkcs8 -topk8 -inform PEM -in saml_private.pem -outform PEM -nocrypt -out saml_private_pkcs8.pem

              show_file saml_private_pkcs8.pem

              # Ensure certificate is in proper PEM format
              openssl x509 -in saml_cert.pem -outform PEM -out saml_cert_formatted.pem

              show_file saml_cert_formatted.pem


              # Create data values file (values.yml)
              cat > values.yml <<EOF
              #@data/values
              ---
              admin_client_secret: ADMIN_PASSWORD
              github_client_id: GITHUB_OIDC_CLIENT_ID
              github_client_secret: GITHUB_OIDC_CLIENT_SECRET
              jwt_private_key_path: "jwt_private.pem"
              saml_private_key_path: "saml_private_pkcs8.pem"
              saml_certificate_path: "saml_cert_formatted.pem"
              uaa_url: http://uaa.uaa.svc.cluster.local:8080
              EOF

              cat > uaa-template.yml <<'EOF'
              #@ load("@ytt:data", "data")
              #@ load("@ytt:yaml", "yaml")
              #@ load("@ytt:base64", "base64")

              ---
              apiVersion: v1
              kind: Namespace
              metadata:
                name: uaa
              ---
              apiVersion: v1
              kind: ServiceAccount
              metadata:
                name: uaa
                namespace: uaa
                labels:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              automountServiceAccountToken: false
              ---
              apiVersion: v1
              kind: Service
              metadata:
                name: uaa
                namespace: uaa
                labels:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              spec:
                type: NodePort
                ports:
                  - port: 8080
                    nodePort: 31000
                    targetPort: http-uaa
                    protocol: TCP
                    name: http-uaa
                selector:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              ---
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                name: uaa
                namespace: uaa
                labels:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app.kubernetes.io/name: uaa
                    app.kubernetes.io/component: authorization_server
                template:
                  metadata:
                    labels:
                      app.kubernetes.io/name: uaa
                      app.kubernetes.io/component: authorization_server
                    annotations:
                      prometheus.io/scrape: 'true'
                      prometheus.io/port: '9102'
                      prometheus.io/path: '/metrics'
                  spec:
                    serviceAccountName: uaa
                    containers:
                      - name: uaa
                        image: cloudfoundry/uaa:77.25.0@sha256:677480cd88dc27bd88d2dc020c8ed640a600d6250b3b0f1f33b0c2d2d613b9e8
                        resources:
                          requests:
                            memory: "512Mi"
                            cpu: "50m"
                          limits:
                            memory: "2000Mi"
                            cpu: "500m"
                        ports:
                          - name: http-uaa
                            containerPort: 8080
                            protocol: TCP
                        env:
                          - name: BPL_TOMCAT_ACCESS_LOGGING
                            value: "y"
                          - name: JAVA_OPTS
                            value: >-
                              -Dspring_profiles=hsqldb
                              -Djava.security.egd=file:/dev/./urandom
                              -Dlogging.config=/etc/config/log4j2.properties
                              -Dlog4j.configurationFile=/etc/config/log4j2.properties
                              -DCLOUDFOUNDRY_CONFIG_PATH=/etc/config
                              -DSECRETS_DIR=/etc/secrets
                              -Dstatsd.enabled=true
                              -Dservlet.session-store=database
                        volumeMounts:
                          - name: uaa-config
                            mountPath: /etc/config
                          - name: uaa-secrets
                            mountPath: /etc/secrets
                        livenessProbe:
                          httpGet:
                            path: /healthz
                            port: http-uaa
                          failureThreshold: 25
                          initialDelaySeconds: 60
                          periodSeconds: 15
                        readinessProbe:
                          httpGet:
                            path: /healthz
                            port: http-uaa
                      - name: statsd-exporter
                        image: "cloudfoundry/statsd_exporter:v0.15.0@sha256:10a64dc4ad0a3e3fe88372f0481dea5c02595c38d168617836a99a649d3ac407"
                        imagePullPolicy: Always
                        args: ["--statsd.listen-udp=:8125"]
                        ports:
                          - name: "metrics-uaa"
                            containerPort: 9102
                            protocol: "TCP"
                        resources:
                          requests:
                            memory: "10Mi"
                            cpu: "10m"
                          limits:
                            memory: "100Mi"
                            cpu: "100m"
                    volumes:
                      - name: uaa-config
                        configMap:
                          name: uaa-config
                      - name: uaa-secrets
                        secret:
                          secretName: uaa-secrets
              ---
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: uaa-config
                namespace: uaa
                labels:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              data:
                log4j2.properties: |
                  status = error
                  dest = err
                  name = UaaLog

                  property.log_pattern=[%d{yyyy-MM-dd'T'HH:mm:ss.nnnnnn}{GMT+0}Z] uaa%X{context} - %pid [%t] .... %5p --- %c{1}: %replace{%m}{(?<=password=|client_secret=)([^&]*)}{<redacted>}%n

                  appender.uaaDefaultAppender.type = Console
                  appender.uaaDefaultAppender.name = UaaDefaultAppender
                  appender.uaaDefaultAppender.layout.type = PatternLayout
                  appender.uaaDefaultAppender.layout.pattern = [UAA] ${log_pattern}

                  appender.uaaAuditAppender.type = Console
                  appender.uaaAuditAppender.name = UaaAuditAppender
                  appender.uaaAuditAppender.layout.type = PatternLayout
                  appender.uaaAuditAppender.layout.pattern = [UAA_AUDIT] ${log_pattern}

                  rootLogger.level = info
                  rootLogger.appenderRef.uaaDefaultAppender.ref = UaaDefaultAppender

                  logger.UAAAudit.name = UAA.Audit
                  logger.UAAAudit.level = info
                  logger.UAAAudit.additivity = true
                  logger.UAAAudit.appenderRef.auditEventLog.ref = UaaAuditAppender

                  logger.cfIdentity.name = org.cloudfoundry.identity
                  logger.cfIdentity.level = info
                  logger.cfIdentity.additivity = false
                  logger.cfIdentity.appenderRef.uaaDefaultAppender.ref = UaaDefaultAppender
                uaa.yml: |
                  issuer:
                    uri: #@ data.values.uaa_url
                  database:
                    maxactive: 100
                    maxidle: 10
                    minidle: 0
                    removeabandoned: false
                    logabandoned: true
                    abandonedtimeout: 300
                    url: "jdbc:hsqldb:mem:uaa"
                  oauth:
                    client:
                      override: true
                    clients:
                      admin:
                        authorized-grant-types: client_credentials
                        authorities: "clients.read,clients.write,clients.secret,uaa.admin,scim.read,scim.write,password.write"
                  login:
                    oauth:
                      providers:
                        github:
                          type: oauth2.0
                          providerDescription: Github OAuth provider, using the 'Authorization Code Grant' flow
                          authUrl: https://github.com/login/oauth/authorize
                          tokenUrl: https://github.com/login/oauth/access_token
                          userInfoUrl: https://api.github.com/user
                          scopes:
                            - read:user
                            - user:email
                          linkText: Login with GitHub
                          showLinkText: true
                          addShadowUserOnLogin: true
                          relyingPartyId: #@ data.values.github_client_id
                          relyingPartySecret: #@ data.values.github_client_secret
                          clientAuthInBody: true
                          skipSslValidation: false
                          attributeMappings:
                            given_name: login
                            family_name: name
                            user_name: email
                    saml:
                      activeKeyId: "saml-key"
                      disableInResponseToCheck: true
                      signMetaData: false
                      signRequest: false
                      wantAssertionSigned: false
              ---
              apiVersion: v1
              kind: Secret
              metadata:
                name: uaa-secrets
                namespace: uaa
                labels:
                  app.kubernetes.io/name: uaa
                  app.kubernetes.io/component: authorization_server
              type: Opaque
              stringData:
                admin_client_credentials.yml: |
                  oauth:
                    clients:
                      admin:
                        secret: #@ data.values.admin_client_secret
                jwt_policy_signing_keys.yml: |
                  jwt:
                    token:
                      policy:
                        activeKeyId: jwt-key
                        keys:
                          jwt-key:
                            signingAlg: RS256
                            signingKey: #@ data.read(data.values.jwt_private_key_path)
                saml_keys.yml: |
                  login:
                    saml:
                      serviceProviderKey: #@ data.read(data.values.saml_private_key_path)
                      serviceProviderCertificate: #@ data.read(data.values.saml_certificate_path)
                      activeKeyId: "saml-key"
                      keys:
                        saml-key:
                          key: #@ data.read(data.values.saml_private_key_path)
                          certificate: #@ data.read(data.values.saml_certificate_path)
              EOF

              show_file values.yml
              show_file uaa-template.yml

              # Combine values and template
              ytt -f values.yml -f uaa-template.yml -f jwt_private.pem -f saml_private_pkcs8.pem -f saml_cert_formatted.pem > uaa-rendered.yml

              show_file uaa-rendered.yml

              # Deploy with kapp
              kapp deploy -a uaa -f uaa-rendered.yml -n uaa --wait-timeout=5m --yes

              kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=uaa -n uaa --timeout=5m
      restartPolicy: Never
      serviceAccountName: korifi-installer
---
apiVersion: batch/v1
kind: Job
metadata:
  name: install-korifi
  namespace: korifi-installer
spec:
  template:
    metadata:
      name: install-korifi
    spec:
      containers:
        - command:
            - bash
            - -c
            - |
              set -euo pipefail
              KORIFI_VERSION="v0.14.0"
              mkdir -p /tmp/korifi
              curl -sSL "https://github.com/cloudfoundry/korifi/releases/download/${KORIFI_VERSION}/install-dependencies.sh" -o /tmp/korifi/scripts/install-dependencies.sh
              chmod +x /tmp/korifi/scripts/install-dependencies.sh
              curl -sSL --create-dirs "https://github.com/cloudfoundry/korifi/releases/download/${KORIFI_VERSION}/tests.tar.gz" -o /tmp/korifi/tests.tar.gz
              tar -xzf /tmp/korifi/tests.tar.gz -C /tmp/korifi/
              rm /tmp/korifi/tests.tar.gz
              cd /tmp/korifi
              scripts/install-dependencies.sh --insecure-tls-metrics-server
              helm repo add twuni https://helm.twun.io
              helm upgrade --install localregistry twuni/docker-registry \
                --namespace default \
                --set service.type=NodePort,service.nodePort=30050,service.port=30050 \
                --set persistence.enabled=true \
                --set persistence.deleteEnabled=true \
                --set secrets.htpasswd='user:$2y$05$Ue5dboOfmqk6Say31Sin9uVbHWTl8J1Sgq9QyAEmFQRnq1TPfP1n2'

              while ! curl -o /dev/null http://localregistry-docker-registry.default.svc.cluster.local:30050/v2/_catalog 2>/dev/null; do
                echo Waiting for the local docker registry to respond...
                sleep 1
              done

              registry_status_code=""
              while [[ "$registry_status_code" != "200" ]]; do
                echo Waiting for the local docker registry to start...
                registry_status_code=$(curl -o /dev/null -w "%{http_code}" --user user:password http://localregistry-docker-registry.default.svc.cluster.local:30050/v2/_catalog 2>/dev/null)
                sleep 1
              done

              helm upgrade --install korifi helm/korifi \
                --namespace korifi \
                --set=adminUserName="kubernetes-admin" \
                --set=defaultAppDomainName="apps-127-0-0-1.nip.io" \
                --set=generateIngressCertificates="true" \
                --set=logLevel="debug" \
                --set=debug="false" \
                --set=stagingRequirements.buildCacheMB="1024" \
                --set=api.apiServer.url="localhost" \
                --set=controllers.taskTTL="5s" \
                --set=jobTaskRunner.jobTTL="5s" \
                --set=containerRepositoryPrefix="localregistry-docker-registry.default.svc.cluster.local:30050/" \
                --set=kpackImageBuilder.builderRepository="localregistry-docker-registry.default.svc.cluster.local:30050/kpack-builder" \
                --set=networking.gatewayClass="contour" \
                --set=networking.gatewayPorts.http="32080" \
                --set=networking.gatewayPorts.https="32443" \
                --set=experimental.managedServices.enabled="true" \
                --set=experimental.managedServices.trustInsecureBrokers="true" \
                --set=experimental.uaa.enabled="true" \
                --set=experimental.uaa.url="http://uaa.uaa.svc.cluster.local:8080" \
                --wait

              kubectl wait --for=condition=ready clusterbuilder --all=true --timeout=15m

              kubectl apply -f - <<EOF
              kind: GatewayClass
              apiVersion: gateway.networking.k8s.io/v1beta1
              metadata:
                name: contour
              spec:
                controllerName: projectcontour.io/gateway-controller
                parametersRef:
                  kind: ContourDeployment
                  group: projectcontour.io
                  name: contour-nodeport-params
                  namespace: projectcontour

              ---
              kind: ContourDeployment
              apiVersion: projectcontour.io/v1alpha1
              metadata:
                namespace: projectcontour
                name: contour-nodeport-params
              spec:
                envoy:
                  networkPublishing:
                    type: NodePortService
              EOF
          image: index.docker.io/cloudfoundry/korifi-installer@sha256:b2a6711b6eeaf2d12d5018ceac6530e4ddc24294c6a16840dae1a8ac2cdba7ac
          name: install-korifi
      restartPolicy: Never
      serviceAccountName: korifi-installer